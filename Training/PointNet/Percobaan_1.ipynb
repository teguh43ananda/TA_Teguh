{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac9984e",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 1 â€“ Import & Config dasar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52f57ca7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import libraries dan konfigurasi dasar\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Untuk split data dan evaluasi\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Untuk deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "# Seed biar hasil lebih konsisten (optional)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Path folder torso per frame (SUSAIKAN DENGAN PUNYAMU)\n",
    "FOLDER_TORSO = r\"E:\\1.Clustering_TA\\3output_clustering\\02_torso_per_frame\"\n",
    "\n",
    "# Mapping subject â†’ label\n",
    "SUBJECT_TO_LABEL = {\n",
    "    \"Afi\": 0,\n",
    "    \"Kinan\": 1,\n",
    "    \"Miftah\": 2,\n",
    "}\n",
    "\n",
    "# Konfigurasi model / dataset\n",
    "FEATURE_NAMES = [\n",
    "    \"torso_x\",\n",
    "    \"torso_y\",\n",
    "    \"torso_z\",\n",
    "    \"vx\",\n",
    "    \"vy\",\n",
    "    \"vz\",\n",
    "    \"is_held_from_prev\",\n",
    "    \"t_norm\",\n",
    "]\n",
    "\n",
    "PAD_VALUE_AFTER_NORM = -10.0  # nilai padding setelah normalisasi (supaya kalah di max-pool)\n",
    "\n",
    "print(\"Config OK.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c10875",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 2 â€“ Helper parsing nama file & baca satu trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e47bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Helper untuk parsing nama file & membaca satu trial\n",
    "\n",
    "def parse_subject_trial_from_filename(fname: str):\n",
    "    \"\"\"\n",
    "    Contoh nama file:\n",
    "    Afi_Jalan10_clustered_torso.csv\n",
    "    Kinan_Jalan72_clustered_torso.csv\n",
    "    \n",
    "    Return:\n",
    "        subject (str), trial_name (str), trial_num (int)\n",
    "    \"\"\"\n",
    "    stem = Path(fname).stem  # Afi_Jalan10_clustered_torso\n",
    "    # Pecah dengan underscore\n",
    "    parts = stem.split(\"_\")  # [\"Afi\", \"Jalan10\", \"clustered\", \"torso\"] (kurang lebih)\n",
    "    if len(parts) < 2:\n",
    "        raise ValueError(f\"Format nama file tidak dikenali: {fname}\")\n",
    "    \n",
    "    subject = parts[0]\n",
    "    trial_raw = parts[1]  # contoh \"Jalan10\"\n",
    "\n",
    "    # Ambil angka di belakang \"Jalan\" untuk trial_num\n",
    "    m = re.search(r\"(\\d+)\", trial_raw)\n",
    "    if m:\n",
    "        trial_num = int(m.group(1))\n",
    "    else:\n",
    "        trial_num = -1  # fallback, kalau tidak ada angka\n",
    "    \n",
    "    trial_name = trial_raw  # simpan apa adanya\n",
    "    \n",
    "    return subject, trial_name, trial_num\n",
    "\n",
    "\n",
    "def load_torso_csv(filepath: str):\n",
    "    \"\"\"\n",
    "    Baca CSV torso per frame dan sort berdasarkan frame.\n",
    "    Pastikan kolom yang dibutuhkan ada.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    required_cols = [\n",
    "        \"frame\",\n",
    "        \"torso_cluster_id\",\n",
    "        \"torso_x\",\n",
    "        \"torso_y\",\n",
    "        \"torso_z\",\n",
    "        \"num_points_torso\",\n",
    "        \"is_held_from_prev\",\n",
    "    ]\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise KeyError(f\"Kolom '{col}' tidak ditemukan di {filepath}\")\n",
    "    \n",
    "    df = df.sort_values(\"frame\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Helper parsing & loader siap.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580dea3",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 3 â€“ Feature engineering per trial (x,y,z,vx,vy,vz,is_held,t_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb827a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Feature engineering untuk satu trial (torso per frame)\n",
    "\n",
    "def compute_frame_features_from_torso_df(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Input: df torso per frame (sudah sorted by frame)\n",
    "    Output: matrix fitur (N_frame Ã— len(FEATURE_NAMES))\n",
    "    \n",
    "    Fitur yang dibuat:\n",
    "      - torso_x, torso_y, torso_z        -> langsung dari df\n",
    "      - vx, vy, vz                        -> diff posisi antar frame (Î” / frame)\n",
    "      - is_held_from_prev                 -> dari df\n",
    "      - t_norm                            -> indeks frame dinormalisasi ke [0, 1]\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        # Tidak ada frame sama sekali\n",
    "        return np.zeros((0, len(FEATURE_NAMES)), dtype=np.float32)\n",
    "    \n",
    "    # Posisi\n",
    "    x = df[\"torso_x\"].to_numpy(dtype=np.float32)\n",
    "    y = df[\"torso_y\"].to_numpy(dtype=np.float32)\n",
    "    z = df[\"torso_z\"].to_numpy(dtype=np.float32)\n",
    "    \n",
    "    # Velocity aproksimasi (Î”pos per frame, mengasumsikan Î”t konstan)\n",
    "    # diff: [x1, x2, x3,...] -> [nan, x2-x1, x3-x2,...]\n",
    "    vx = np.diff(x, prepend=x[0])\n",
    "    vy = np.diff(y, prepend=y[0])\n",
    "    vz = np.diff(z, prepend=z[0])\n",
    "    \n",
    "    vx = vx.astype(np.float32)\n",
    "    vy = vy.astype(np.float32)\n",
    "    vz = vz.astype(np.float32)\n",
    "    \n",
    "    # is_held_from_prev -> float (0.0 atau 1.0)\n",
    "    is_held = df[\"is_held_from_prev\"].to_numpy(dtype=np.float32)\n",
    "    \n",
    "    # t_norm: 0..1 sepanjang trial\n",
    "    N = len(df)\n",
    "    if N > 1:\n",
    "        t_norm = np.linspace(0.0, 1.0, N, dtype=np.float32)\n",
    "    else:\n",
    "        t_norm = np.array([0.0], dtype=np.float32)\n",
    "    \n",
    "    # Susun matrix fitur dalam urutan FEATURE_NAMES\n",
    "    features_list = []\n",
    "    \n",
    "    for name in FEATURE_NAMES:\n",
    "        if name == \"torso_x\":\n",
    "            features_list.append(x)\n",
    "        elif name == \"torso_y\":\n",
    "            features_list.append(y)\n",
    "        elif name == \"torso_z\":\n",
    "            features_list.append(z)\n",
    "        elif name == \"vx\":\n",
    "            features_list.append(vx)\n",
    "        elif name == \"vy\":\n",
    "            features_list.append(vy)\n",
    "        elif name == \"vz\":\n",
    "            features_list.append(vz)\n",
    "        elif name == \"is_held_from_prev\":\n",
    "            features_list.append(is_held)\n",
    "        elif name == \"t_norm\":\n",
    "            features_list.append(t_norm)\n",
    "        else:\n",
    "            raise KeyError(f\"FEATURE_NAMES berisi nama yang tidak dikenali: {name}\")\n",
    "    \n",
    "    mat = np.stack(features_list, axis=1)  # shape = (N_frame, n_features)\n",
    "    return mat.astype(np.float32)\n",
    "\n",
    "\n",
    "print(\"Fungsi feature engineering per trial siap.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3596b8",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 4 â€“ Kumpulkan semua trial (X_list & metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc33f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load semua file torso, bangun list matrix fitur + metadata\n",
    "\n",
    "all_trial_features = []  # list of np.ndarray, shape variabel (N_frame_i, n_features)\n",
    "all_subjects = []        # list of subject string\n",
    "all_labels = []          # list of int label\n",
    "all_trial_names = []     # e.g., \"Jalan10\"\n",
    "all_trial_nums = []      # int\n",
    "\n",
    "folder_path = Path(FOLDER_TORSO)\n",
    "csv_files = sorted(folder_path.glob(\"*_clustered_torso.csv\"))\n",
    "\n",
    "print(f\"Total file ditemukan: {len(csv_files)}\")\n",
    "\n",
    "for fpath in csv_files:\n",
    "    subject, trial_name, trial_num = parse_subject_trial_from_filename(fpath.name)\n",
    "    if subject not in SUBJECT_TO_LABEL:\n",
    "        print(f\"[WARNING] Subject '{subject}' tidak dikenali, skip file: {fpath.name}\")\n",
    "        continue\n",
    "    \n",
    "    df_torso = load_torso_csv(fpath)\n",
    "    \n",
    "    # Kalau df kosong, skip\n",
    "    if df_torso.empty:\n",
    "        print(f\"[WARNING] Data frame kosong, skip file: {fpath.name}\")\n",
    "        continue\n",
    "    \n",
    "    feat_mat = compute_frame_features_from_torso_df(df_torso)\n",
    "    if feat_mat.shape[0] == 0:\n",
    "        print(f\"[WARNING] Hasil fitur 0 frame, skip file: {fpath.name}\")\n",
    "        continue\n",
    "    \n",
    "    all_trial_features.append(feat_mat)\n",
    "    all_subjects.append(subject)\n",
    "    all_labels.append(SUBJECT_TO_LABEL[subject])\n",
    "    all_trial_names.append(trial_name)\n",
    "    all_trial_nums.append(trial_num)\n",
    "\n",
    "print(\"Total trial valid:\", len(all_trial_features))\n",
    "print(\"Contoh shape trial pertama:\", all_trial_features[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b0f6a",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 5 â€“ Hitung N_MAX_FRAMES & build normalisasi global (Z-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e8e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Analisis panjang sequence & hitung parameter normalisasi global\n",
    "\n",
    "# Panjang tiap trial\n",
    "lengths = np.array([m.shape[0] for m in all_trial_features], dtype=np.int32)\n",
    "print(\"Statistik panjang frame per trial:\")\n",
    "print(\"  min  :\", lengths.min())\n",
    "print(\"  max  :\", lengths.max())\n",
    "print(\"  mean :\", lengths.mean())\n",
    "print(\"  90th :\", np.percentile(lengths, 90))\n",
    "print(\"  95th :\", np.percentile(lengths, 95))\n",
    "\n",
    "# Pilih N_MAX_FRAMES berdasarkan percentile (boleh kamu ubah kalau mau eksplisit)\n",
    "N_MAX_FRAMES = int(np.clip(np.percentile(lengths, 95), 64, 512))\n",
    "print(\"N_MAX_FRAMES yang dipakai:\", N_MAX_FRAMES)\n",
    "\n",
    "# Hitung Z-score global dari semua frame di semua trial (sebelum padding)\n",
    "# Gabungkan semua frame real\n",
    "all_frames_concat = np.concatenate(all_trial_features, axis=0)  # shape (sum_N, n_features)\n",
    "feature_means = all_frames_concat.mean(axis=0)\n",
    "feature_stds = all_frames_concat.std(axis=0) + 1e-8  # tambah epsilon supaya tidak 0\n",
    "\n",
    "print(\"Feature means:\", feature_means)\n",
    "print(\"Feature stds :\", feature_stds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0ac82",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 6 â€“ Fungsi pad/crop & bangun X, y final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1358fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Fungsi padding/cropping & membangun X, y dalam bentuk numpy array\n",
    "\n",
    "def normalize_features(mat: np.ndarray, means: np.ndarray, stds: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Z-score global: (x - mean) / std, feature-wise.\n",
    "    \"\"\"\n",
    "    return (mat - means) / stds\n",
    "\n",
    "\n",
    "def pad_or_crop_sequence(mat: np.ndarray, N_max: int, pad_value: float = PAD_VALUE_AFTER_NORM) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        mat: (N_frame, n_features) sesudah normalisasi\n",
    "        N_max: panjang frame target\n",
    "    Output:\n",
    "        out: (N_max, n_features)\n",
    "    \n",
    "    - Jika N_frame > N_max: center crop (ambil tengah)\n",
    "    - Jika N_frame < N_max: pad di akhir dengan pad_value\n",
    "    \"\"\"\n",
    "    N, D = mat.shape\n",
    "    if N == N_max:\n",
    "        return mat\n",
    "    \n",
    "    if N > N_max:\n",
    "        # center crop\n",
    "        start = (N - N_max) // 2\n",
    "        end = start + N_max\n",
    "        return mat[start:end, :]\n",
    "    \n",
    "    # N < N_max: pad di tail\n",
    "    out = np.full((N_max, D), pad_value, dtype=mat.dtype)\n",
    "    out[:N, :] = mat\n",
    "    return out\n",
    "\n",
    "\n",
    "# Bangun X dan y\n",
    "n_trials = len(all_trial_features)\n",
    "n_features = len(FEATURE_NAMES)\n",
    "\n",
    "X = np.zeros((n_trials, N_MAX_FRAMES, n_features), dtype=np.float32)\n",
    "y = np.array(all_labels, dtype=np.int32)\n",
    "\n",
    "for i, mat in enumerate(all_trial_features):\n",
    "    mat_norm = normalize_features(mat, feature_means, feature_stds)\n",
    "    X[i] = pad_or_crop_sequence(mat_norm, N_MAX_FRAMES, pad_value=PAD_VALUE_AFTER_NORM)\n",
    "\n",
    "print(\"Shape X:\", X.shape)  # (n_trial, N_max_frames, n_features)\n",
    "print(\"Shape y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9c59f",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 7 â€“ Train/Val/Test split (70/15/15, stratified subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a456e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Split train/val/test 70/15/15 (stratified by subject/label)\n",
    "\n",
    "# Pertama: train (70%) vs temp (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.30,\n",
    "    random_state=SEED,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "# Kedua: temp (30%) dipecah jadi val (15%) dan test (15%)\n",
    "# 0.5 * 30% = 15%\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.50,\n",
    "    random_state=SEED,\n",
    "    stratify=y_temp,\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Val   shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Test  shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Cek distribusi label per split\n",
    "def label_dist(name, arr):\n",
    "    unique, counts = np.unique(arr, return_counts=True)\n",
    "    print(f\"\\n{name} label distribution:\")\n",
    "    for u, c in zip(unique, counts):\n",
    "        print(f\"  label {u}: {c}\")\n",
    "\n",
    "label_dist(\"Train\", y_train)\n",
    "label_dist(\"Val\", y_val)\n",
    "label_dist(\"Test\", y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba74637",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 8 â€“ Definisi model PointNet-style (spatial only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f858d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Definisi model PointNet-style (per-frame MLP + global max pooling)\n",
    "\n",
    "def build_pointnet_spatial_model(\n",
    "    n_frames: int,\n",
    "    n_features: int,\n",
    "    n_classes: int = 3,\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    n_frames   : N_MAX_FRAMES\n",
    "    n_features : len(FEATURE_NAMES)\n",
    "    n_classes  : jumlah kelas (Afi/Kinan/Miftah = 3)\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(n_frames, n_features), name=\"spatial_input\")\n",
    "    \n",
    "    # Shared per-frame MLP (TimeDistributed Dense)\n",
    "    x = layers.TimeDistributed(layers.Dense(64, activation=\"relu\"))(inputs)\n",
    "    x = layers.TimeDistributed(layers.BatchNormalization())(x)\n",
    "    \n",
    "    x = layers.TimeDistributed(layers.Dense(128, activation=\"relu\"))(x)\n",
    "    x = layers.TimeDistributed(layers.BatchNormalization())(x)\n",
    "    \n",
    "    x = layers.TimeDistributed(layers.Dense(256, activation=\"relu\"))(x)\n",
    "    x = layers.TimeDistributed(layers.BatchNormalization())(x)\n",
    "    \n",
    "    # Global max pooling di axis frame\n",
    "    x = layers.GlobalMaxPooling1D()(x)  # -> (batch, 256)\n",
    "    \n",
    "    # Head klasifikasi\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    outputs = layers.Dense(n_classes, activation=\"softmax\", name=\"class_output\")(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name=\"PointNet_Spatial_Only\")\n",
    "    return model\n",
    "\n",
    "\n",
    "n_classes = len(SUBJECT_TO_LABEL)\n",
    "model = build_pointnet_spatial_model(\n",
    "    n_frames=N_MAX_FRAMES,\n",
    "    n_features=len(FEATURE_NAMES),\n",
    "    n_classes=n_classes,\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3448dacf",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 9 â€“ Compile & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21078155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Compile & training model\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 16\n",
    "n_epochs = 50\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Callback: early stopping + save best model\n",
    "checkpoint_path = \"pointnet_spatial_only_best.h5\"\n",
    "\n",
    "cb_early = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=8,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "cb_ckpt = callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[cb_early, cb_ckpt],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Training selesai. Best model disimpan ke:\", checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f183f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "train_loss = history_dict.get(\"loss\", [])\n",
    "val_loss   = history_dict.get(\"val_loss\", [])\n",
    "train_acc  = history_dict.get(\"accuracy\", [])\n",
    "val_acc    = history_dict.get(\"val_accuracy\", [])\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ---- Plot loss ----\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, label=\"Train Loss\")\n",
    "plt.plot(epochs, val_loss, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend()\n",
    "\n",
    "# ---- Plot accuracy ----\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_acc, label=\"Train Acc\")\n",
    "plt.plot(epochs, val_acc, label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training & Validation Accuracy\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8105d0",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 10 â€“ Evaluasi di test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75366db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Evaluasi di test set\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test acc : {test_acc:.4f}\")\n",
    "\n",
    "# Prediksi & laporan klasifikasi\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(\"\\nClassification report (label 0=Afi, 1=Kinan, 2=Miftah):\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "labels_order = [0, 1, 2]\n",
    "label_names = [\"Afi\", \"Kinan\", \"Miftah\"]\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels_order)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "disp.plot(ax=ax, cmap=\"Blues\", colorbar=True)\n",
    "plt.title(\"Confusion Matrix â€“ Test Set\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eece2b96",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 11 â€“ Simpan model final (opsional jika mau override)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a93454",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = \"pointnet_spatial_only_final.h5\"\n",
    "model.save(final_model_path)\n",
    "print(\"Model final disimpan ke:\", final_model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39513ecd",
   "metadata": {},
   "source": [
    "Cell 1 — Imports + config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72d3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "np.random.seed(42)  # reproducible (walau analisis deterministic)\n",
    "\n",
    "# =========================\n",
    "# PATHS (sesuaikan)\n",
    "# =========================\n",
    "BASE_HEAD2_DIR = Path(r\"E:\\0.TA_Teguh\\GMM Trial 2\\Head 2\")  # Head 2 output (per subject folder, per trial file)\n",
    "SUBJECTS = list(\"ABCDEFGHIJ\")\n",
    "\n",
    "# =========================\n",
    "# TRIAL-2 params (untuk valid fallback)\n",
    "# =========================\n",
    "MIN_POINTS = 5   \n",
    "\n",
    "print(\"BASE_HEAD2_DIR:\", BASE_HEAD2_DIR)\n",
    "print(\"SUBJECTS      :\", SUBJECTS)\n",
    "print(\"MIN_POINTS    :\", MIN_POINTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024d92a",
   "metadata": {},
   "source": [
    "Cell 2 — Utility: list files Head-2 per subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d8326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_head2_files(subject: str):\n",
    "    \"\"\"\n",
    "    List all Head-2 CSV files for a subject.\n",
    "    Expected pattern: BASE_HEAD2_DIR/<subject>/Jalan*.csv\n",
    "    \"\"\"\n",
    "    subj_dir = BASE_HEAD2_DIR / subject\n",
    "    pattern = str(subj_dir / \"Jalan*.csv\")\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    return files\n",
    "\n",
    "# quick sanity check\n",
    "for s in SUBJECTS:\n",
    "    files = list_head2_files(s)\n",
    "    print(f\"{s}: {len(files)} files, example: {files[0] if files else 'NONE'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ccf22e",
   "metadata": {},
   "source": [
    "Cell 3 — Load Head-2 into one DataFrame (frame-level), with dedup + schema normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021bba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pick_col(df: pd.DataFrame, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def load_all_head2(subjects=SUBJECTS):\n",
    "    rows = []\n",
    "    dup_reports = []\n",
    "\n",
    "    for subj in subjects:\n",
    "        files = list_head2_files(subj)\n",
    "        if not files:\n",
    "            print(f\"[WARN] No Head-2 files for subject {subj}\")\n",
    "            continue\n",
    "\n",
    "        for fpath in files:\n",
    "            fpath = Path(fpath)\n",
    "            trial_name = fpath.stem  # \"Jalan12\"\n",
    "            try:\n",
    "                df = pd.read_csv(fpath)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed reading {fpath}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # ---- required columns (try to be robust)\n",
    "            col_frame = _pick_col(df, [\"frame\", \"Frame\", \"frame_id\"])\n",
    "            col_nroi  = _pick_col(df, [\"N_roi\", \"n_roi\", \"N_ROI\", \"nroi\"])\n",
    "            col_valid = _pick_col(df, [\"valid_minpts\", \"valid\", \"is_valid\"])\n",
    "            col_nin   = _pick_col(df, [\"N_inlier\", \"n_inlier\", \"N_INLIER\", \"ninlier\", \"nInlier\"])\n",
    "            col_conf  = _pick_col(df, [\"conf\", \"confidence\", \"conf_frame\"])\n",
    "\n",
    "            if col_frame is None:\n",
    "                print(f\"[WARN] Missing 'frame' in {fpath.name}, skip\")\n",
    "                continue\n",
    "\n",
    "            # build normalized frame-level table\n",
    "            out = pd.DataFrame()\n",
    "            out[\"subject\"] = subj\n",
    "            out[\"trial\"]   = trial_name\n",
    "            out[\"frame\"]   = pd.to_numeric(df[col_frame], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "            # optional numeric columns\n",
    "            if col_nroi is not None:\n",
    "                out[\"N_roi\"] = pd.to_numeric(df[col_nroi], errors=\"coerce\")\n",
    "            else:\n",
    "                out[\"N_roi\"] = np.nan\n",
    "\n",
    "            if col_nin is not None:\n",
    "                out[\"N_inlier\"] = pd.to_numeric(df[col_nin], errors=\"coerce\")\n",
    "            else:\n",
    "                # if not found, we can't do N_target analysis properly\n",
    "                out[\"N_inlier\"] = np.nan\n",
    "\n",
    "            if col_conf is not None:\n",
    "                out[\"conf\"] = pd.to_numeric(df[col_conf], errors=\"coerce\")\n",
    "            else:\n",
    "                out[\"conf\"] = np.nan\n",
    "\n",
    "            # validity: prefer explicit valid_minpts, otherwise fallback using N_roi >= MIN_POINTS\n",
    "            if col_valid is not None:\n",
    "                v = pd.to_numeric(df[col_valid], errors=\"coerce\")\n",
    "                out[\"valid_minpts\"] = (v.fillna(0).astype(int) > 0).astype(int)\n",
    "            else:\n",
    "                out[\"valid_minpts\"] = ((out[\"N_roi\"].fillna(0) >= MIN_POINTS)).astype(int)\n",
    "\n",
    "            rows.append(out)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    all_df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    # drop rows with missing frame\n",
    "    all_df = all_df.dropna(subset=[\"frame\"]).copy()\n",
    "    all_df[\"frame\"] = all_df[\"frame\"].astype(int)\n",
    "\n",
    "    # ---- DUPLICATE CHECK: (subject, trial, frame) must be unique\n",
    "    key = [\"subject\", \"trial\", \"frame\"]\n",
    "    dup_mask = all_df.duplicated(key, keep=False)\n",
    "    if dup_mask.any():\n",
    "        dups = all_df.loc[dup_mask, key].value_counts().reset_index(name=\"count\")\n",
    "        dup_reports = dups.sort_values(\"count\", ascending=False)\n",
    "        print(f\"[WARN] Found duplicates for (subject, trial, frame): {len(dup_reports)} keys duplicated\")\n",
    "        # keep last occurrence\n",
    "        all_df = all_df.sort_values(key).drop_duplicates(key, keep=\"last\").reset_index(drop=True)\n",
    "        print(\"[INFO] Duplicates dropped (keep='last').\")\n",
    "\n",
    "    dup_reports_df = pd.DataFrame(dup_reports) if isinstance(dup_reports, list) else dup_reports\n",
    "    return all_df, dup_reports_df\n",
    "\n",
    "df_h2, df_dups = load_all_head2(SUBJECTS)\n",
    "\n",
    "print(\"df_h2 shape:\", df_h2.shape)\n",
    "df_h2.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f78f80",
   "metadata": {},
   "source": [
    "Cell 4 — Sanity checks (missing columns, NaNs, duplicates report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e482ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates report (if any)\n",
    "if isinstance(df_dups, pd.DataFrame) and not df_dups.empty:\n",
    "    display(df_dups.head(20))\n",
    "\n",
    "# how many NaNs in N_inlier?\n",
    "nan_nin = df_h2[\"N_inlier\"].isna().sum()\n",
    "print(\"NaN N_inlier:\", nan_nin, \"out of\", len(df_h2))\n",
    "\n",
    "# if too many NaNs, stop (analysis would be meaningless)\n",
    "if nan_nin > 0:\n",
    "    print(\"[WARN] Some rows have NaN N_inlier. Those frames will be excluded from sampling pressure calc.\")\n",
    "\n",
    "# valid/invalid counts\n",
    "print(df_h2[\"valid_minpts\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9c43b",
   "metadata": {},
   "source": [
    "Cell 5 — Build point-count dataset for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fec05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows where N_inlier is known\n",
    "df_counts = df_h2.dropna(subset=[\"N_inlier\"]).copy()\n",
    "df_counts[\"N_inlier\"] = df_counts[\"N_inlier\"].astype(int)\n",
    "\n",
    "# Two views:\n",
    "# - ALL frames (including invalid) -> useful to understand how many invalid exist\n",
    "# - VALID frames only -> the ONLY one that should drive N_target decisions\n",
    "df_all   = df_counts.copy()\n",
    "df_valid = df_counts[df_counts[\"valid_minpts\"] == 1].copy()\n",
    "\n",
    "print(\"ALL frames used   :\", len(df_all))\n",
    "print(\"VALID frames used :\", len(df_valid))\n",
    "display(df_valid.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60de652c",
   "metadata": {},
   "source": [
    "Cell 6 — Descriptive stats (global + per subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_points(arr: np.ndarray):\n",
    "    arr = np.asarray(arr)\n",
    "    return {\n",
    "        \"min\": float(np.min(arr)),\n",
    "        \"max\": float(np.max(arr)),\n",
    "        \"mean\": float(np.mean(arr)),\n",
    "        \"median\": float(np.median(arr)),\n",
    "        \"p25\": float(np.percentile(arr, 25)),\n",
    "        \"p50\": float(np.percentile(arr, 50)),\n",
    "        \"p75\": float(np.percentile(arr, 75)),\n",
    "        \"p90\": float(np.percentile(arr, 90)),\n",
    "        \"p95\": float(np.percentile(arr, 95)),\n",
    "        \"p99\": float(np.percentile(arr, 99)),\n",
    "    }\n",
    "\n",
    "def print_stats(title, arr):\n",
    "    st = describe_points(arr)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    for k, v in st.items():\n",
    "        print(f\"  {k:6s}: {v:.3f}\")\n",
    "    return st\n",
    "\n",
    "# GLOBAL\n",
    "stats_all = print_stats(\"GLOBAL (ALL frames)\", df_all[\"N_inlier\"].values)\n",
    "stats_val = print_stats(\"GLOBAL (VALID frames only)\", df_valid[\"N_inlier\"].values)\n",
    "\n",
    "# PER SUBJECT (VALID only)\n",
    "print(\"\\n=== PER SUBJECT (VALID frames only) ===\")\n",
    "per_subject_stats = []\n",
    "for s in SUBJECTS:\n",
    "    sub = df_valid[df_valid[\"subject\"] == s]\n",
    "    if len(sub) == 0:\n",
    "        print(f\"Subject {s}: NO VALID data\")\n",
    "        continue\n",
    "    st = describe_points(sub[\"N_inlier\"].values)\n",
    "    per_subject_stats.append({\"subject\": s, **st, \"n_frames\": len(sub)})\n",
    "    print(f\"\\nSubject {s} (VALID): n={len(sub)}\")\n",
    "    for k, v in st.items():\n",
    "        print(f\"  {k:6s}: {v:.3f}\")\n",
    "\n",
    "df_per_subject_stats = pd.DataFrame(per_subject_stats)\n",
    "df_per_subject_stats.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8efe8c",
   "metadata": {},
   "source": [
    "Cell 7 — Auto-generate candidate N_target (data-driven)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate generator around p50/p75/p90 of VALID frames\n",
    "p50 = int(round(stats_val[\"p50\"]))\n",
    "p75 = int(round(stats_val[\"p75\"]))\n",
    "p90 = int(round(stats_val[\"p90\"]))\n",
    "\n",
    "# Make a small grid around those percentiles\n",
    "def around(x, steps=( -4, 0, 4 )):\n",
    "    out = []\n",
    "    for d in steps:\n",
    "        v = x + d\n",
    "        if v > 0:\n",
    "            out.append(v)\n",
    "    return out\n",
    "\n",
    "N_candidates = [32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112]\n",
    "\n",
    "\n",
    "print(\"Auto N_candidates (from VALID p50/p75/p90):\", N_candidates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184bc4e8",
   "metadata": {},
   "source": [
    "Cell 8 — Simulate sampling pressure for each N_target (VALID only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafedc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_sampling_pressure(df_valid: pd.DataFrame, n_targets):\n",
    "    \"\"\"\n",
    "    df_valid must already be VALID frames only and have column N_inlier (int).\n",
    "    For each N_target:\n",
    "      - fill: frames needing upsampling (N_inlier < N_target)\n",
    "      - down: frames needing downsampling (N_inlier > N_target)\n",
    "      - equal\n",
    "    Also compute deficit/excess statistics for practical \"pressure\" evaluation.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    M = df_valid[\"N_inlier\"].values.astype(int)\n",
    "    total = len(M)\n",
    "\n",
    "    for N in n_targets:\n",
    "        need_fill = M < N\n",
    "        need_down = M > N\n",
    "        equal     = M == N\n",
    "\n",
    "        n_fill = int(need_fill.sum())\n",
    "        n_down = int(need_down.sum())\n",
    "        n_eq   = int(equal.sum())\n",
    "\n",
    "        # deficits/excess\n",
    "        deficits = (N - M[need_fill]) if n_fill > 0 else np.array([])\n",
    "        excess   = (M[need_down] - N) if n_down > 0 else np.array([])\n",
    "\n",
    "        rec = {\n",
    "            \"N_target\": int(N),\n",
    "            \"total_frames_used\": int(total),\n",
    "            \"n_need_fill\": n_fill,\n",
    "            \"n_equal\": n_eq,\n",
    "            \"n_need_down\": n_down,\n",
    "            \"pct_need_fill\": 100.0 * n_fill / total if total else 0.0,\n",
    "            \"pct_equal\": 100.0 * n_eq / total if total else 0.0,\n",
    "            \"pct_need_down\": 100.0 * n_down / total if total else 0.0,\n",
    "            \"avg_deficit\": float(deficits.mean()) if deficits.size else 0.0,\n",
    "            \"p95_deficit\": float(np.percentile(deficits, 95)) if deficits.size else 0.0,\n",
    "            \"avg_excess\": float(excess.mean()) if excess.size else 0.0,\n",
    "            \"p95_excess\": float(np.percentile(excess, 95)) if excess.size else 0.0,\n",
    "        }\n",
    "        records.append(rec)\n",
    "\n",
    "    return pd.DataFrame(records).sort_values(\"N_target\").reset_index(drop=True)\n",
    "\n",
    "df_pressure = simulate_sampling_pressure(df_valid, N_candidates)\n",
    "display(df_pressure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8794a3b",
   "metadata": {},
   "source": [
    "Cell 9 — (Optional) Compare pressure on ALL frames vs VALID frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ae3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: ALL frames includes invalid, but still uses N_inlier column\n",
    "# If your invalid frames have N_inlier=0 (or NaN), statistics will shift.\n",
    "df_all_validlike = df_all.copy()  # could include invalid\n",
    "df_all_validlike = df_all_validlike.dropna(subset=[\"N_inlier\"])\n",
    "df_all_validlike[\"N_inlier\"] = df_all_validlike[\"N_inlier\"].astype(int)\n",
    "\n",
    "# \"valid_only=False\" effect: pressure may look very different\n",
    "df_pressure_all = simulate_sampling_pressure(df_all_validlike, N_candidates)\n",
    "\n",
    "print(\"=== Pressure (VALID frames only) ===\")\n",
    "display(df_pressure)\n",
    "\n",
    "print(\"=== Pressure (ALL frames, incl invalid) ===\")\n",
    "display(df_pressure_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16125bfc",
   "metadata": {},
   "source": [
    "Cell 10 — Quick plots (histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0639cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(df_valid[\"N_inlier\"].values, bins=60)\n",
    "plt.title(\"Distribution of N_inlier (VALID frames only)\")\n",
    "plt.xlabel(\"N_inlier\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7e52f",
   "metadata": {},
   "source": [
    "Cell 11 — Save outputs for audit trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69cfaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = Path(r\"E:\\0.TA_Teguh\\Clustering GMM\") / \"_analysis_n_target\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_per_subject_stats.to_csv(OUT_DIR / \"per_subject_stats_valid.csv\", index=False)\n",
    "df_pressure.to_csv(OUT_DIR / \"sampling_pressure_valid.csv\", index=False)\n",
    "\n",
    "print(\"Saved to:\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gait-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

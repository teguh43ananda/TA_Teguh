{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb70744",
   "metadata": {},
   "source": [
    " GMM Trial 1 (Baseline): Hard SNR Gating\n",
    " Dataset: TI IWR6843AOP point cloud (x,y,z,doppler,SNR)\n",
    " Goal:\n",
    "   - Head 1: point-level p_torso + component_id (CSV)\n",
    "   - Head 2: frame-level conf + stats (CSV)\n",
    "   - Head 3: window-level GMM params (JSONL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835f0aad",
   "metadata": {},
   "source": [
    "CELL 2 — Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d2e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from scipy.stats import chi2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "# Optional interactivity (VSCode Jupyter)\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    HAS_WIDGETS = True\n",
    "except Exception:\n",
    "    HAS_WIDGETS = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3a4ae",
   "metadata": {},
   "source": [
    "CELL 3 — Config (ALL parameters in one place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a28a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "DATASET_ROOT = Path(r\"E:\\0.TA_Teguh\\dataset3\")   # contains A..J\n",
    "OUT_ROOT     = Path(r\"E:\\0.TA_Teguh\\GMM\")        # output root\n",
    "\n",
    "SUBJECTS = list(\"ABCDEFGHIJ\")  # A..J\n",
    "TRIALS = list(range(1, 73))    # Jalan1..Jalan72\n",
    "\n",
    "# ===============================\n",
    "# Trial-2 Configuration (LOCKED)\n",
    "# ===============================\n",
    "\n",
    "# ROI (NO Z GATING)\n",
    "ROI_X = (-2.5, 2.5)\n",
    "ROI_Y = (0.0, 5.0)\n",
    "\n",
    "# Frame validity\n",
    "MIN_POINTS = 5   # baseline (later try 10)\n",
    "\n",
    "# Gaussian model\n",
    "GMM_K = 1        # SINGLE Gaussian (K=1)\n",
    "\n",
    "# Mahalanobis outlier removal\n",
    "CHI2_LEVEL = 0.99   # baseline\n",
    "CHI2_DF = 3         # x,y,z\n",
    "\n",
    "# Refit\n",
    "USE_REFIT_OFFLINE = True\n",
    "\n",
    "# Soft SNR weighting (baseline global)\n",
    "SNR_REF = 10.0      # reference SNR for scaling\n",
    "SNR_WEIGHT_MIN = 0.1\n",
    "SNR_WEIGHT_MAX = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d06d97d",
   "metadata": {},
   "source": [
    "CELL 4 — Output folder preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870be1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dirs():\n",
    "    # Head folders\n",
    "    for head_name in [\"Head 1\", \"Head 2\", \"Head 3\"]:\n",
    "        (OUT_ROOT / head_name).mkdir(parents=True, exist_ok=True)\n",
    "        (OUT_ROOT / head_name / \"_summary\").mkdir(parents=True, exist_ok=True)\n",
    "        for s in SUBJECTS:\n",
    "            (OUT_ROOT / head_name / s).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ensure_dirs()\n",
    "print(\"Output folders ready at:\", OUT_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbbc794",
   "metadata": {},
   "source": [
    "CELL 5 — File indexing (build a table of 720 paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_file_index() -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for s in SUBJECTS:\n",
    "        for t in TRIALS:\n",
    "            in_path = DATASET_ROOT / s / f\"Jalan{t}.csv\"\n",
    "            h1_out = OUT_ROOT / \"Head 1\" / s / f\"Jalan{t}.csv\"\n",
    "            h2_out = OUT_ROOT / \"Head 2\" / s / f\"Jalan{t}.csv\"\n",
    "            h3_out = OUT_ROOT / \"Head 3\" / s / f\"Jalan{t}.jsonl\"\n",
    "            rows.append({\n",
    "                \"subject\": s,\n",
    "                \"trial\": t,\n",
    "                \"input_path\": str(in_path),\n",
    "                \"head1_out\": str(h1_out),\n",
    "                \"head2_out\": str(h2_out),\n",
    "                \"head3_out\": str(h3_out),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "index_df = build_file_index()\n",
    "\n",
    "# Validate missing files\n",
    "missing = index_df[~index_df[\"input_path\"].apply(lambda p: Path(p).exists())]\n",
    "print(\"Total files:\", len(index_df))\n",
    "print(\"Missing files:\", len(missing))\n",
    "if len(missing) > 0:\n",
    "    display(missing.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22ad1a9",
   "metadata": {},
   "source": [
    "CELL 6 — Load one CSV (strict schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f7de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_COLS = [\"timestamp\", \"frame\", \"x\", \"y\", \"z\", \"doppler\", \"SNR\"]\n",
    "\n",
    "def load_trial_csv(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    # Validate schema\n",
    "    for c in REQUIRED_COLS:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Missing column '{c}' in {path}\")\n",
    "\n",
    "    # Type casting\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].astype(str)\n",
    "    df[\"frame\"] = df[\"frame\"].astype(int)\n",
    "    for c in [\"x\", \"y\", \"z\", \"doppler\", \"SNR\"]:\n",
    "        df[c] = df[c].astype(float)\n",
    "\n",
    "    # Sort\n",
    "    df = df.sort_values([\"frame\"]).reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69e9394",
   "metadata": {},
   "source": [
    "CELL 7 — Soft SNR Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0823fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_snr_weight(snr_values):\n",
    "    \"\"\"\n",
    "    Soft SNR weighting (global, clipped)\n",
    "    \"\"\"\n",
    "    w = snr_values / SNR_REF\n",
    "    w = np.clip(w, SNR_WEIGHT_MIN, SNR_WEIGHT_MAX)\n",
    "    return w\n",
    "\n",
    "def fit_gaussian_k1(X, weights=None):\n",
    "    if weights is None:\n",
    "        mu = X.mean(axis=0)\n",
    "        Sigma = np.cov(X.T)\n",
    "    else:\n",
    "        w = np.asarray(weights, dtype=float)\n",
    "        w = w / (np.sum(w) + 1e-12)\n",
    "\n",
    "        mu = np.sum(X * w[:, None], axis=0)\n",
    "        Xc = X - mu\n",
    "        Sigma = (Xc.T * w) @ Xc\n",
    "\n",
    "    # Numerical safety\n",
    "    Sigma += 1e-6 * np.eye(3)\n",
    "    return mu, Sigma\n",
    "\n",
    "\n",
    "def mahalanobis_filter(X, mu, Sigma, chi2_level):\n",
    "    \"\"\"\n",
    "    Compute Mahalanobis d² and inlier mask\n",
    "    \"\"\"\n",
    "    invSigma = np.linalg.inv(Sigma)\n",
    "    diff = X - mu\n",
    "    d2 = np.einsum(\"ij,jk,ik->i\", diff, invSigma, diff)\n",
    "\n",
    "    threshold = chi2.ppf(chi2_level, df=CHI2_DF)\n",
    "    inlier_mask = d2 <= threshold\n",
    "    return d2, inlier_mask, threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b6875a",
   "metadata": {},
   "source": [
    "Cell 8 - Procces one File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_file(input_path, head1_out, head2_out, head3_out):\n",
    "\n",
    "    df = load_trial_csv(input_path)\n",
    "\n",
    "    head1_rows = []\n",
    "    head2_rows = []\n",
    "    head3_rows = []\n",
    "\n",
    "    points_before = len(df)\n",
    "    frames = sorted(df[\"frame\"].unique())\n",
    "\n",
    "    for f in frames:\n",
    "        df_f = df[df[\"frame\"] == f]\n",
    "        ts = df_f[\"timestamp\"].iloc[0]\n",
    "\n",
    "        # ROI XY only\n",
    "        df_roi = df_f[\n",
    "            (df_f[\"x\"].between(*ROI_X)) &\n",
    "            (df_f[\"y\"].between(*ROI_Y))\n",
    "        ]\n",
    "\n",
    "        N_raw = len(df_f)\n",
    "        N_roi = len(df_roi)\n",
    "\n",
    "        summary = {\n",
    "            \"timestamp\": ts,\n",
    "            \"frame\": f,\n",
    "            \"N_raw\": N_raw,\n",
    "            \"N_roi\": N_roi,\n",
    "            \"minpts_ok\": int(N_roi >= MIN_POINTS),\n",
    "            \"valid_frame\": 0,\n",
    "            \"N_inlier\": 0,\n",
    "            \"conf\": 0.0,\n",
    "            \"centroid_x\": np.nan,\n",
    "            \"centroid_y\": np.nan,\n",
    "            \"centroid_z\": np.nan,\n",
    "        }\n",
    "\n",
    "        if N_roi < MIN_POINTS:\n",
    "            head2_rows.append(summary)\n",
    "            continue\n",
    "\n",
    "        X = df_roi[[\"x\", \"y\", \"z\"]].values\n",
    "        snr = df_roi[\"SNR\"].values\n",
    "        w = compute_snr_weight(snr)\n",
    "\n",
    "        mu, Sigma = fit_gaussian_k1(X, w)\n",
    "        d2, inlier_mask, chi2_thr = mahalanobis_filter(X, mu, Sigma, CHI2_LEVEL)\n",
    "\n",
    "        df_in = df_roi[inlier_mask].copy()\n",
    "        df_in[\"md2\"] = d2[inlier_mask]\n",
    "        df_in[\"w_snr\"] = w[inlier_mask]\n",
    "\n",
    "        if len(df_in) < MIN_POINTS:\n",
    "            head2_rows.append(summary)\n",
    "            continue\n",
    "\n",
    "        # Optional refit\n",
    "        if USE_REFIT_OFFLINE:\n",
    "            mu, Sigma = fit_gaussian_k1(\n",
    "                df_in[[\"x\",\"y\",\"z\"]].values,\n",
    "                df_in[\"w_snr\"].values\n",
    "            )\n",
    "\n",
    "        # Update summary\n",
    "        summary.update({\n",
    "            \"valid_frame\": 1,\n",
    "            \"N_inlier\": len(df_in),\n",
    "            \"conf\": len(df_in) / max(N_roi, 1),\n",
    "            \"centroid_x\": mu[0],\n",
    "            \"centroid_y\": mu[1],\n",
    "            \"centroid_z\": mu[2],\n",
    "        })\n",
    "\n",
    "        head2_rows.append(summary)\n",
    "\n",
    "        # Head 1 (Option A)\n",
    "        head1_rows.append(df_in.assign(frame=f, timestamp=ts))\n",
    "\n",
    "        # Head 3\n",
    "        head3_rows.append({\n",
    "            \"timestamp\": ts,\n",
    "            \"frame\": f,\n",
    "            \"mu\": mu.tolist(),\n",
    "            \"Sigma\": Sigma.tolist(),\n",
    "            \"chi2_threshold\": float(chi2_thr),\n",
    "            \"N_roi\": int(N_roi),\n",
    "            \"N_inlier\": int(len(df_in)),\n",
    "        })\n",
    "\n",
    "    # SAVE\n",
    "    if head1_rows:\n",
    "        pd.concat(head1_rows).to_csv(head1_out, index=False)\n",
    "    else:\n",
    "        pd.DataFrame().to_csv(head1_out, index=False)\n",
    "\n",
    "    pd.DataFrame(head2_rows).to_csv(head2_out, index=False)\n",
    "\n",
    "    with open(head3_out, \"w\") as f:\n",
    "        for r in head3_rows:\n",
    "            f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "    return {\n",
    "        \"input_path\": input_path,\n",
    "        \"points_before\": points_before,\n",
    "        \"points_after\": sum(r[\"N_inlier\"] for r in head2_rows),\n",
    "        \"frames_total\": len(frames),\n",
    "        \"frames_invalid\": sum(1 for r in head2_rows if r[\"valid_frame\"] == 0),\n",
    "        \"mean_conf\": np.mean([r[\"conf\"] for r in head2_rows]),\n",
    "        \"min_conf\": np.min([r[\"conf\"] for r in head2_rows]),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5e605",
   "metadata": {},
   "source": [
    "CELL 9 — Batch runner (A..J, Jalan1..72) + per-subject summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5704fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(index_df: pd.DataFrame, subjects: Optional[List[str]] = None, trials: Optional[List[int]] = None) -> pd.DataFrame:\n",
    "    if subjects is not None:\n",
    "        df = index_df[index_df[\"subject\"].isin(subjects)].copy()\n",
    "    else:\n",
    "        df = index_df.copy()\n",
    "\n",
    "    if trials is not None:\n",
    "        df = df[df[\"trial\"].isin(trials)].copy()\n",
    "\n",
    "    summaries = []\n",
    "    total = len(df)\n",
    "    for i, row in df.reset_index(drop=True).iterrows():\n",
    "        s = row[\"subject\"]\n",
    "        t = int(row[\"trial\"])\n",
    "        print(f\"[{i+1}/{total}] Processing {s}/Jalan{t} ...\")\n",
    "\n",
    "        try:\n",
    "            summ = process_one_file(\n",
    "                input_path=row[\"input_path\"],\n",
    "                head1_out=row[\"head1_out\"],\n",
    "                head2_out=row[\"head2_out\"],\n",
    "                head3_out=row[\"head3_out\"]\n",
    "            )\n",
    "            summ[\"subject\"] = s\n",
    "            summ[\"trial\"] = t\n",
    "            summaries.append(summ)\n",
    "        except Exception as e:\n",
    "            print(\"  ERROR:\", e)\n",
    "            summaries.append({\n",
    "                \"subject\": s, \"trial\": t,\n",
    "                \"input_path\": row[\"input_path\"],\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ec290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Safety: start with 1 subject, 1 trial to verify\n",
    "#summaries_df = run_batch(index_df, subjects=[\"A\"], trials=[1])\n",
    "summaries_df = run_batch(index_df, subjects=list(\"ABCDEFGHIJ\"), trials=list(range(1,73)))\n",
    "display(summaries_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_subject_summaries(summaries_df: pd.DataFrame):\n",
    "    # Head 1 summary folder\n",
    "    h1_sum_dir = OUT_ROOT / \"Head 1\" / \"_summary\"\n",
    "    h2_sum_dir = OUT_ROOT / \"Head 2\" / \"_summary\"\n",
    "    h3_sum_dir = OUT_ROOT / \"Head 3\" / \"_summary\"\n",
    "\n",
    "    for s in SUBJECTS:\n",
    "        sub = summaries_df[summaries_df[\"subject\"] == s].copy()\n",
    "        if len(sub) == 0:\n",
    "            continue\n",
    "\n",
    "        # Minimal summary CSVs\n",
    "        cols_common = [\n",
    "            \"subject\", \"trial\",\n",
    "            \"points_before\", \"points_after\",\n",
    "            \"frames_total\", \"frames_invalid\",\n",
    "            \"mean_conf\", \"min_conf\",\n",
    "            \"input_path\"\n",
    "        ]\n",
    "\n",
    "        sub_out = sub[[c for c in cols_common if c in sub.columns]].copy()\n",
    "\n",
    "        sub_out.to_csv(h1_sum_dir / f\"summary_{s}.csv\", index=False)\n",
    "        sub_out.to_csv(h2_sum_dir / f\"summary_{s}.csv\", index=False)\n",
    "        sub_out.to_csv(h3_sum_dir / f\"summary_{s}.csv\", index=False)\n",
    "\n",
    "    # Optional global summary\n",
    "    (OUT_ROOT / \"_summary\").mkdir(exist_ok=True)\n",
    "    summaries_df.to_csv(OUT_ROOT / \"_summary\" / \"global_summary.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gait-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0412cd3",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 1 â€“ Import + Konfigurasi Path & HPO Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3584ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DBSCAN\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import dan konfigurasi dasar\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === PATH DASAR (SESUAI STRUKTUR KAMU) ===\n",
    "\n",
    "BASE_DIR = r\"E:\\0.TA_Teguh\\dataset2\"\n",
    "\n",
    "# Folder raw (di dalamnya ada folder per subjek: Afi, Tsania, Tsamara)\n",
    "RAW_DIR = BASE_DIR  # contoh: E:\\0.TA_Teguh\\dataset2\\Afi\\Jalan1.csv\n",
    "\n",
    "# Folder output clustering dan torso (per subjek)\n",
    "CLUSTER_DIR = os.path.join(BASE_DIR, \"hasil_clustering\")\n",
    "TORSO_DIR   = os.path.join(BASE_DIR, \"hasil_torso\")  # dipakai nanti untuk branch torso\n",
    "\n",
    "# Daftar subjek (folder) yang akan diproses\n",
    "SUBJECTS = [\"Afi\", \"Tsania\", \"Tsamara\"]\n",
    "\n",
    "# Pastikan folder hasil_clustering & hasil_torso + subfolder per subjek ada\n",
    "for subj in SUBJECTS:\n",
    "    os.makedirs(os.path.join(CLUSTER_DIR, subj), exist_ok=True)\n",
    "    os.makedirs(os.path.join(TORSO_DIR,   subj), exist_ok=True)\n",
    "\n",
    "# === MODE PERLAKUAN FRAME KURUS ===\n",
    "# Jika titik dalam 1 frame < min_samples dan > 0 â†’ mau dianggap 1 cluster kecil atau noise semua?\n",
    "SMALL_FRAME_AS_SINGLE_CLUSTER = True\n",
    "\n",
    "# === GRID PARAMETER UNTUK HPO (cari eps & min_samples terbaik) ===\n",
    "EPS_CANDIDATES         = [0.25, 0.30, 0.32, 0.35, 0.40]\n",
    "MIN_SAMPLES_CANDIDATES = [3, 5, 8]\n",
    "\n",
    "# Berapa banyak file sampel per subjek untuk HPO\n",
    "HPO_MAX_FILES_PER_SUBJECT = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4601de",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 2 â€“ Utility: List File Mentah Per Subjek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e75436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Fungsi untuk mengambil semua file raw per subjek\n",
    "\n",
    "def list_raw_files_for_subject(subject_name: str):\n",
    "    \"\"\"\n",
    "    Mengembalikan list path penuh ke semua file CSV mentah\n",
    "    untuk satu subjek, misal:\n",
    "    E:\\0.TA_Teguh\\dataset2\\Afi\\Jalan1.csv, Jalan2.csv, dst.\n",
    "    \"\"\"\n",
    "    subj_dir = os.path.join(RAW_DIR, subject_name)\n",
    "    pattern = os.path.join(subj_dir, \"*.csv\")\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    return files\n",
    "\n",
    "# (Opsional) Cek cepat isi per subjek\n",
    "for subj in SUBJECTS:\n",
    "    files = list_raw_files_for_subject(subj)\n",
    "    print(f\"Subjek {subj}: {len(files)} file ditemukan\")\n",
    "    for f in files:\n",
    "        print(\"  \", os.path.basename(f))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260448e6",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 3 â€“ DBSCAN untuk Satu Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8525db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: DBSCAN untuk 1 frame\n",
    "\n",
    "def run_dbscan_on_frame(df_frame: pd.DataFrame,\n",
    "                        eps: float,\n",
    "                        min_samples: int,\n",
    "                        small_frame_as_single_cluster: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Menerima subset DataFrame untuk 1 frame (kolom x, y, z),\n",
    "    mengembalikan array cluster_id untuk tiap titik di frame itu.\n",
    "    \n",
    "    - Jika jumlah titik >= min_samples â†’ jalankan DBSCAN normal.\n",
    "    - Jika 0 titik â†’ kembalikan array kosong.\n",
    "    - Jika 0 < jumlah titik < min_samples:\n",
    "        - Jika small_frame_as_single_cluster=True â†’ semua titik cluster 0.\n",
    "        - Jika False â†’ semua titik noise (-1).\n",
    "    \"\"\"\n",
    "    if df_frame.empty:\n",
    "        return np.array([], dtype=int)\n",
    "    \n",
    "    coords = df_frame[[\"x\", \"y\", \"z\"]].values\n",
    "    n_points = coords.shape[0]\n",
    "    \n",
    "    if n_points >= min_samples:\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = db.fit_predict(coords)\n",
    "        return labels.astype(int)\n",
    "    else:\n",
    "        if small_frame_as_single_cluster:\n",
    "            labels = np.zeros(n_points, dtype=int)   # semua cluster 0\n",
    "        else:\n",
    "            labels = -np.ones(n_points, dtype=int)   # semua noise\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe98a764",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 4 â€“ Evaluasi Param DBSCAN untuk 1 File (HPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49abcc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Evaluasi kualitas DBSCAN untuk 1 file (tanpa menyimpan CSV)\n",
    "\n",
    "def evaluate_dbscan_params_on_file(input_path: str,\n",
    "                                   eps: float,\n",
    "                                   min_samples: int,\n",
    "                                   small_frame_as_single_cluster: bool = True):\n",
    "    \"\"\"\n",
    "    Jalankan DBSCAN per frame untuk satu file, lalu hitung metrik:\n",
    "    - total_frames\n",
    "    - frames_with_non_noise : frame yang punya cluster_id != -1\n",
    "    - coverage              : frames_with_non_noise / total_frames\n",
    "    - avg_clusters_per_frame: rata-rata jumlah cluster non-noise per frame\n",
    "    - avg_non_noise_points  : rata-rata jumlah titik non-noise per frame\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_path)\n",
    "    if \"frame\" not in df.columns:\n",
    "        raise ValueError(\"Kolom 'frame' tidak ditemukan di file: \" + input_path)\n",
    "    \n",
    "    frame_groups = df.groupby(\"frame\").groups\n",
    "    total_frames = len(frame_groups)\n",
    "    \n",
    "    frames_with_non_noise = 0\n",
    "    clusters_count_list = []\n",
    "    non_noise_points_list = []\n",
    "    \n",
    "    for frame_id, idx in frame_groups.items():\n",
    "        df_frame = df.loc[idx]\n",
    "        labels = run_dbscan_on_frame(\n",
    "            df_frame,\n",
    "            eps=eps,\n",
    "            min_samples=min_samples,\n",
    "            small_frame_as_single_cluster=small_frame_as_single_cluster\n",
    "        )\n",
    "        \n",
    "        if labels.size == 0:\n",
    "            clusters_count_list.append(0)\n",
    "            non_noise_points_list.append(0)\n",
    "            continue\n",
    "        \n",
    "        non_noise_mask = (labels != -1)\n",
    "        non_noise_labels = labels[non_noise_mask]\n",
    "        \n",
    "        if non_noise_labels.size > 0:\n",
    "            frames_with_non_noise += 1\n",
    "            unique_clusters = np.unique(non_noise_labels)\n",
    "            clusters_count_list.append(len(unique_clusters))\n",
    "            non_noise_points_list.append(non_noise_labels.size)\n",
    "        else:\n",
    "            clusters_count_list.append(0)\n",
    "            non_noise_points_list.append(0)\n",
    "    \n",
    "    coverage = frames_with_non_noise / total_frames if total_frames > 0 else 0.0\n",
    "    avg_clusters_per_frame = np.mean(clusters_count_list) if clusters_count_list else 0.0\n",
    "    avg_non_noise_points = np.mean(non_noise_points_list) if non_noise_points_list else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"file\": input_path,\n",
    "        \"eps\": eps,\n",
    "        \"min_samples\": min_samples,\n",
    "        \"total_frames\": total_frames,\n",
    "        \"frames_with_non_noise\": frames_with_non_noise,\n",
    "        \"coverage\": coverage,\n",
    "        \"avg_clusters_per_frame\": avg_clusters_per_frame,\n",
    "        \"avg_non_noise_points\": avg_non_noise_points,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa89be",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 5 â€“ HPO: Coba Semua Kombinasi eps Ã— min_samples di Beberapa File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaf4fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: HPO sederhana untuk DBSCAN di beberapa file sampel\n",
    "\n",
    "def run_hpo_dbscan(sample_subjects=None,\n",
    "                   max_files_per_subject=HPO_MAX_FILES_PER_SUBJECT):\n",
    "    \"\"\"\n",
    "    Jalankan evaluasi DBSCAN untuk grid eps x min_samples\n",
    "    di beberapa file sampel dari tiap subjek.\n",
    "    \n",
    "    sample_subjects: list subjek yang mau diikutkan (default: SUBJECTS)\n",
    "    \"\"\"\n",
    "    if sample_subjects is None:\n",
    "        sample_subjects = SUBJECTS\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for subj in sample_subjects:\n",
    "        raw_files = list_raw_files_for_subject(subj)\n",
    "        if not raw_files:\n",
    "            print(f\"[HPO] Tidak ada file untuk subjek {subj}\")\n",
    "            continue\n",
    "        \n",
    "        sample_files = raw_files[:max_files_per_subject]\n",
    "        print(f\"[HPO] Subjek {subj}, sample files:\")\n",
    "        for f in sample_files:\n",
    "            print(\"    \", os.path.basename(f))\n",
    "        print()\n",
    "        \n",
    "        for fpath in sample_files:\n",
    "            for eps in EPS_CANDIDATES:\n",
    "                for ms in MIN_SAMPLES_CANDIDATES:\n",
    "                    metrics = evaluate_dbscan_params_on_file(\n",
    "                        input_path=fpath,\n",
    "                        eps=eps,\n",
    "                        min_samples=ms,\n",
    "                        small_frame_as_single_cluster=SMALL_FRAME_AS_SINGLE_CLUSTER\n",
    "                    )\n",
    "                    metrics[\"subject\"] = subj\n",
    "                    results.append(metrics)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"Tidak ada hasil HPO (cek kembali folder dan SUBJECTS).\")\n",
    "        return None, None\n",
    "    \n",
    "    df_res = pd.DataFrame(results)\n",
    "    \n",
    "    # Skor sederhana:\n",
    "    # score = coverage - penalty * |avg_clusters_per_frame - 1|\n",
    "    penalty = 0.3\n",
    "    df_res[\"score\"] = df_res[\"coverage\"] - penalty * (df_res[\"avg_clusters_per_frame\"] - 1).abs()\n",
    "    \n",
    "    df_res_sorted = df_res.sort_values(by=[\"score\", \"coverage\"], ascending=False)\n",
    "    \n",
    "    return df_res, df_res_sorted\n",
    "\n",
    "# Contoh pemanggilan HPO (fase 1):\n",
    "# df_hpo, df_hpo_sorted = run_hpo_dbscan()\n",
    "# df_hpo_sorted.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd498379",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 6 â€“ Set Parameter Final DBSCAN dari Hasil HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Set parameter DBSCAN FINAL berdasarkan hasil HPO\n",
    "\n",
    "# Contoh: misalkan dari df_hpo_sorted kamu pilih eps=0.30 dan min_samples=5\n",
    "DBSCAN_EPS_FINAL         = 0.30    # ganti sesuai hasil HPO-mu\n",
    "DBSCAN_MIN_SAMPLES_FINAL = 5       # ganti sesuai hasil HPO-mu\n",
    "\n",
    "print(\"Parameter DBSCAN final:\")\n",
    "print(\"  eps        =\", DBSCAN_EPS_FINAL)\n",
    "print(\"  min_samples=\", DBSCAN_MIN_SAMPLES_FINAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a31d81",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 7 â€“ Proses 1 File: Tambah cluster_id dan Simpan ke hasil_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e4dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Proses 1 file raw â†’ simpan file clustering (dengan cluster_id)\n",
    "\n",
    "def build_output_cluster_path(subject_name: str, input_file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Dari path input, misal:\n",
    "        E:\\0.TA_Teguh\\dataset2\\Afi\\Jalan1.csv\n",
    "    hasilkan path output:\n",
    "        E:\\0.TA_Teguh\\dataset2\\hasil_clustering\\Afi\\clustering_Jalan1.csv\n",
    "    \"\"\"\n",
    "    fname = os.path.basename(input_file_path)      # \"Jalan1.csv\"\n",
    "    output_fname = f\"clustering_{fname}\"          # \"clustering_Jalan1.csv\"\n",
    "    subj_out_dir = os.path.join(CLUSTER_DIR, subject_name)\n",
    "    return os.path.join(subj_out_dir, output_fname)\n",
    "\n",
    "\n",
    "def process_single_file_dbscan(input_path: str,\n",
    "                               output_path: str,\n",
    "                               eps: float,\n",
    "                               min_samples: int,\n",
    "                               small_frame_as_single_cluster: bool = True):\n",
    "    \"\"\"\n",
    "    Membaca file CSV mentah (timestamp, frame, x, y, z, doppler, SNR),\n",
    "    melakukan DBSCAN per frame, menambahkan kolom cluster_id,\n",
    "    lalu menyimpannya ke output_path.\n",
    "    \"\"\"\n",
    "    print(f\"Memproses file: {input_path}\")\n",
    "    df = pd.read_csv(input_path)\n",
    "    \n",
    "    if \"frame\" not in df.columns:\n",
    "        raise ValueError(\"Kolom 'frame' tidak ditemukan dalam file: \" + input_path)\n",
    "    \n",
    "    cluster_ids = np.full(len(df), -1, dtype=int)  # default noise\n",
    "    \n",
    "    for frame_id, idx in df.groupby(\"frame\").groups.items():\n",
    "        df_frame = df.loc[idx]\n",
    "        labels = run_dbscan_on_frame(\n",
    "            df_frame,\n",
    "            eps=eps,\n",
    "            min_samples=min_samples,\n",
    "            small_frame_as_single_cluster=small_frame_as_single_cluster\n",
    "        )\n",
    "        cluster_ids[list(idx)] = labels\n",
    "    \n",
    "    df[\"cluster_id\"] = cluster_ids\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"  >> Disimpan ke: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608c7aef",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 8 â€“ Jalankan DBSCAN untuk Semua Subjek & Semua File (Fase Produksi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c7fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Jalankan DBSCAN final untuk semua subjek & semua file raw\n",
    "\n",
    "def run_dbscan_for_all_subjects(eps_final: float,\n",
    "                                min_samples_final: int,\n",
    "                                small_frame_as_single_cluster: bool = True):\n",
    "    \"\"\"\n",
    "    Menggunakan parameter final hasil HPO,\n",
    "    jalankan DBSCAN untuk semua file raw semua subjek,\n",
    "    dan simpan hasil ke folder hasil_clustering/<subjek>/clustering_JalanN.csv\n",
    "    \"\"\"\n",
    "    for subj in SUBJECTS:\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Subjek: {subj}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        raw_files = list_raw_files_for_subject(subj)\n",
    "        if not raw_files:\n",
    "            print(f\"  Tidak ada file CSV untuk subjek {subj}\")\n",
    "            continue\n",
    "        \n",
    "        for raw_path in raw_files:\n",
    "            out_path = build_output_cluster_path(subj, raw_path)\n",
    "            process_single_file_dbscan(\n",
    "                input_path=raw_path,\n",
    "                output_path=out_path,\n",
    "                eps=eps_final,\n",
    "                min_samples=min_samples_final,\n",
    "                small_frame_as_single_cluster=small_frame_as_single_cluster\n",
    "            )\n",
    "        print()\n",
    "\n",
    "# Contoh pemanggilan (FASE 2, setelah HPO & set param final):\n",
    "# run_dbscan_for_all_subjects(DBSCAN_EPS_FINAL, DBSCAN_MIN_SAMPLES_FINAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74972ad0",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 9 (Opsional) â€“ Ringkasan Cepat 1 File Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d852c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 (opsional): Ringkasan hasil clustering satu file\n",
    "\n",
    "def summarize_cluster_file(cluster_file_path: str, max_frames: int = 5):\n",
    "    \"\"\"\n",
    "    Tampilkan ringkasan singkat hasil clustering:\n",
    "    - jumlah baris total\n",
    "    - distribusi cluster_id\n",
    "    - beberapa frame awal: jumlah titik per cluster\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(cluster_file_path)\n",
    "    print(f\"File: {cluster_file_path}\")\n",
    "    print(f\"Total baris: {len(df)}\")\n",
    "    print(\"Distribusi cluster_id:\")\n",
    "    print(df[\"cluster_id\"].value_counts().sort_index())\n",
    "    print()\n",
    "    \n",
    "    frame_ids = sorted(df[\"frame\"].unique())[:max_frames]\n",
    "    for f in frame_ids:\n",
    "        sub = df[df[\"frame\"] == f]\n",
    "        print(f\"Frame {f}: {len(sub)} titik\")\n",
    "        print(sub[\"cluster_id\"].value_counts().sort_index())\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# Contoh:\n",
    "# contoh_file = r\"E:\\0.TA_Teguh\\dataset2\\hasil_clustering\\Afi\\clustering_Jalan1.csv\"\n",
    "# summarize_cluster_file(contoh_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7084debe",
   "metadata": {},
   "source": [
    "ðŸ§© Cell 10 (Opsional) â€“ Visualisasi 2D Satu Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005991d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 (opsional): Visualisasi cluster untuk satu frame (2D)\n",
    "\n",
    "def plot_clusters_for_frame(cluster_file_path: str, frame_id: int):\n",
    "    \"\"\"\n",
    "    Plot 2D (x vs y) titik untuk satu frame tertentu, diwarnai berdasarkan cluster_id.\n",
    "    Noise (-1) â†’ abu-abu.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(cluster_file_path)\n",
    "    df_f = df[df[\"frame\"] == frame_id]\n",
    "    \n",
    "    if df_f.empty:\n",
    "        print(f\"Tidak ada data untuk frame {frame_id}\")\n",
    "        return\n",
    "    \n",
    "    cluster_ids = df_f[\"cluster_id\"].unique()\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for cid in sorted(cluster_ids):\n",
    "        sub = df_f[df_f[\"cluster_id\"] == cid]\n",
    "        if cid == -1:\n",
    "            label = \"noise (-1)\"\n",
    "            alpha = 0.3\n",
    "        else:\n",
    "            label = f\"cluster {cid}\"\n",
    "            alpha = 0.8\n",
    "        plt.scatter(sub[\"x\"], sub[\"y\"], label=label, alpha=alpha)\n",
    "    \n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(f\"{os.path.basename(cluster_file_path)} - frame {frame_id}\")\n",
    "    plt.legend()\n",
    "    plt.axis(\"equal\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Contoh:\n",
    "# plot_clusters_for_frame(contoh_file, frame_id=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
